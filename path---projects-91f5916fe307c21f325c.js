webpackJsonp([97786326051841],{365:function(e,a){e.exports={data:{site:{siteMetadata:{title:"Laura Domine",subtitle:"(aka Temigo)",copyright:"Powered by Gatsby. © All rights reserved.",menu:[{label:"Articles",path:"/"},{label:"Projects",path:"/projects"},{label:"About",path:"/about/"},{label:"Contact",path:"/contact/"}],author:{name:"Laura Domine",email:"ldomine@stanford.edu",telegram:"#",twitter:"#",github:"Temigo",rss:"#",vk:"#"}}},markdownRemark:{id:"/home/mai-hien/hummingbird2/src/pages/pages/2018-05-13--projects/index.md absPath of file >>> MarkdownRemark",html:'<p>I enjoy working on these besides my research.</p>\n<h2>Yup’ik Eskimo and Machine Translation for Low-Resource Polysynthetic Languages</h2>\n<p><em>Jan. 2018 - ongoing, with Christopher W. Liu</em></p>\n<p>Won 1st prize (based on the reports) for the course project of <a href="http://cs224n.stanford.edu">CS224n - Natural Language Processing with Deep Learning</a>.</p>\n<p>Machine translation tools do not yet exist for the Yup’ik Eskimo language. It is a low-resource language spoken by around 8,000 people who primarily live in Southwest Alaska. With the availability of Yup’ik Eskimo / English parallel text (~100k sentences), we developed a pipeline for reliable translation of this language pair.</p>\n<p>We wrote a morphological rule-based parser for the Yup’ik Eskimo language and compared it with other unsupervised tokenization methods. We trained a bidirectional LSTM model with attention and reached a BLEU score of 13 using Byte-Pair Encoding, an unsupervised tokenization method.</p>\n<p>We are now working on the development of <strong>Yuarcuun</strong>, a translation and dictionary tool for Yup’ik Eskimo which will be available online.</p>\n<p><a href="http://web.stanford.edu/class/cs224n/reports/6907893.pdf">Final Report</a> -\n<a href="https://github.com/cwtliu/yupik-mt">Code</a></p>\n<h2>Cloud Removal in Hyperspectral Satellite Images using Generative Adversarial Networks</h2>\n<p><em>Apr. 2018 - ongoing</em></p>\n<p>Won Best Solo prize and Best Satellogic data use at <a href="https://bigearthhacks.stanford.edu/">Stanford Big Earth Hackathon</a>.</p>\n<p>Satellite imagery can be used to monitor the environment or predict disasters and enable quick responses. Clouds bring uneven illumination, blurring and occlusion of the target. Satellite images are traditionally multispectral, i.e. include only a few wavelengths. Hyperspectral images include numerous wavelengths, including near-infrared, and are becoming more widely available. Finally, Generative Adversarial Networks (GANs) are among the most successful unsupervised techniques for generating realistic images by training 2 networks in competition (generator vs discriminator).</p>\n<p>I proposed to apply GANs to hyperspectral satellite images in order to generate the missing patches from below the clouds. I devised and implemented the whole data pre-processing pipeline which includes classifying satellite images patches as cloudy/clear and synthesizing cloud masks.</p>\n<p>Project still ongoing for <a href="http://cs231n.stanford.edu">CS231n - Convolutional Neural Networks for Visual Recognition</a>.</p>\n<p><a href="https://devpost.com/software/kumo-san">DevPost</a></p>',frontmatter:{title:"Projects",date:null,description:null}}},pathContext:{slug:"/projects"}}}});
//# sourceMappingURL=path---projects-91f5916fe307c21f325c.js.map